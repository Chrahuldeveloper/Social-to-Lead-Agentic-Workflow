# Knowledge-Based AI Agent (RAG System)

This project implements a **knowledge-grounded AI agent** that answers user questions using a **local knowledge base** via **Retrieval-Augmented Generation (RAG)**.

The agent retrieves relevant context from stored documents and generates answers strictly based on that context using an LLM.

---

## âœ¨ Key Features

- Local knowledge base ingestion (Markdown)
- Chunking & embeddings pipeline
- Vector search using ChromaDB
- Agent reasoning & intent detection
- Context-grounded LLM responses
- Clean modular architecture
- Hot-reload support for development
---

## ğŸ§  Agent Capabilities

The agent:
- Detects user intent
- Retrieves relevant knowledge using vector similarity
- Generates answers **only from retrieved context**
- Responds with *â€œI donâ€™t knowâ€* when information is missing

---

## ğŸ“ Project Structure

```

â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ agent.py          
â”‚   â””â”€â”€ state.py         
â”‚
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ vectorstore.py
|   â””â”€â”€ generator.py       
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ knowledge.md      
â”‚
â”œâ”€â”€ main.py               
â”œâ”€â”€ config.py               
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

---

## ğŸ“š Knowledge Base

Stored locally in `data/knowledge.md`.

Example content:
- Pricing & feature plans
- Company policies
- Support details

The file is automatically:
1. Chunked
2. Embedded
3. Stored in a vector database

---

## ğŸ”„ RAG Pipeline

1. Load `knowledge.md`
2. Split into semantic chunks
3. Generate embeddings (Sentence Transformers)
4. Store in ChromaDB
5. Retrieve top-K relevant chunks
6. Inject context into LLM prompt
7. Generate grounded answer

---

## ğŸ¤– Agent Workflow

```

User Question
â†“
Intent Detection
â†“
Vector Retrieval (RAG)
â†“
Context Assembly
â†“
LLM Generation
â†“
Final Answer

````

---

## ğŸ§© Task Requirements Mapping

### 1. Agent reasoning & intent detection
- Implemented via `AgentState`
- Routes queries through correct execution path

### 2. Correct use of RAG
- Vector search performed **before** LLM call
- LLM answers strictly from retrieved context

### 3. Clean state management
- `AgentState` object tracks question, intent, context, answer

### 4. Proper tool calling logic
- Retrieval and generation are clearly separated
- No direct LLM calls without context

### 5. Code clarity & structure
- Modular folder-based architecture
- Single responsibility per file

### 6. Real-world deployability
- Local vector DB persistence
- Hot-reload support
- Easily extensible to API

---

## ğŸ›  Tech Stack

- **Python 3.10+**
- **SentenceTransformers** â€“ embeddings
- **ChromaDB** â€“ vector database
- **LangChain Text Splitters**
- **Gemini** â€“ answer generation

---

## â–¶ï¸ Running the Project

### 1. Install dependencies
```bash
pip install -r requirements.txt
````

### 2. Start the agent (with hot reload)

```bash
python main.py
```

---

## ğŸ§ª Example Query

```text
User: What are the plans available?
Agent:
- Basic Plan: $29/month, 10 videos, 720p
- Pro Plan: $79/month, unlimited videos, 4K, AI captions
```

---

## ğŸš€ Future Improvements

* Streaming responses
* Conversation memory
* REST API / FastAPI interface
* UI dashboard
* Multi-document support

---

## ğŸ“ Notes

* The agent **does not hallucinate**
* Answers are strictly grounded in local knowledge
* Safe fallback behavior for unknown queries

---

## ğŸ“Œ Summary

This project demonstrates a **production-ready AI agent** using best practices in:

* RAG
* Agent architecture
* Vector databases
* Clean state management

Designed to be **simple, reliable, and extensible**.

```


